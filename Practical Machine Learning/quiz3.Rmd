---
title: "Quiz 3"
author: "Giuliano Vesci"
date: "Tuesday, August 18, 2015"
output: html_document
---

#Question 1 
```{r}
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)

set.seed(125)

training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
modFit <- train(Class ~ ., method="rpart", data=training)
modFit$finalModel

# a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2 => PS
# b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100 => WS
# c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100 => PS
# d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2 => Not possible to predict 

```

#Question 2
```{r}

# If K is small in a K-fold cross validation is the bias in the estimate of out-of-sample (test set) accuracy smaller or bigger? If K is small is the variance in the estimate of out-of-sample (test set) accuracy smaller or bigger. Is K large or small in leave one out cross validation?

# 1. The bias is smaller and the variance is smaller. Under leave one out cross validation K is equal to the sample size.

# 2. The bias is smaller and the variance is bigger. Under leave one out cross validation K is equal to one.

# 3. The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to two.

# 4. The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size.

# 4
```


#Question 3 
```{r}
library(pgmm)
data(olive)
olive = olive[,-1]

## olive dataset downladed
modFit <- train(Area  ~ ., method="rpart", data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata) # 2.875
# 2.783. It is strange because Area should be a qualitative 
# variable - but tree is reporting the average value of Area as 
# a numeric variable in the leaf predicted for newdata

```

#Question 4 
```{r}
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]

set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl ,data=trainSA,method="glm", family="binomial")

missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}

predictionsTest <- predict(modFit, testSA)
missClass(testSA$chd, predictionsTest) # 0.31

predictionsTrain <- predict(modFit, trainSA)
missClass(trainSA$chd, predictionsTrain) # 0.31


```


#Question 5 
```{r}

library(ElemStatLearn)
data(vowel.train)
data(vowel.test) 



```